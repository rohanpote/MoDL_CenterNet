{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F \n",
    "import torch.utils.data as td\n",
    "import torchvision as tv\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "import skimage.io as io\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(sys.path[0]+'/src/lib') # Add library folder\n",
    "sys.path.append(sys.path[0]+'/src/lib/models/networks/DCNv2')\n",
    "from models.model import create_model, load_model, save_model\n",
    "from trains.ctdet import CtdetTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opts import opts\n",
    "from datasets.dataset_factory import get_dataset\n",
    "from datasets.dataset.coco import COCO\n",
    "from datasets.sample.ctdet import CTDetDataset\n",
    "from trains.ctdet import CtdetTrainer\n",
    "from trains.train_factory import train_factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(sys.path[0]+'/src/project_tools')\n",
    "from fcn_opts import fcn_opts\n",
    "Dataset = get_dataset('coco', 'ctdet')\n",
    "opt = fcn_opts(Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of object categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_class_name = [\n",
    "     'person', 'bicycle', 'car', 'motorcycle', 'airplane',\n",
    "     'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant',\n",
    "     'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n",
    "     'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack',\n",
    "     'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n",
    "     'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
    "     'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass',\n",
    "     'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n",
    "     'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',\n",
    "     'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv',\n",
    "     'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave',\n",
    "     'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase',\n",
    "     'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 1 (Pre-Trained vs Our Baseline model with 2048 training samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_dir=sys.path[0]+'/Test_images/'\n",
    "img_dir=sys.path[0]+'/images/'\n",
    "# tst_id=1210\n",
    "# im_id = valset[tst_id]['meta']['img_id']\n",
    "# img = valset.coco.loadImgs(im_id)[0]\n",
    "# f_name=img_dir\n",
    "# f_name+=img['file_name']\n",
    "f_name='1.jpg'\n",
    "f_name=img_dir+f_name\n",
    "I = io.imread(f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(sys.path[0]+'/src/lib/models/networks/DCNv2')\n",
    "from detectors.ctdet import CtdetDetector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting model parameters using opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.load_model = sys.path[0]+'/models/ctdet_coco_resdcn18.pth' #Base model path\n",
    "opt.dataset = 'coco' #type of dataset\n",
    "opt.debugger_theme = 'white'\n",
    "opt.flip_test = False\n",
    "opt.K = 100 #maximum number of detections\n",
    "opt.nms = False #Non-maximal suppresion\n",
    "opt.vis_thresh = 0.3 #Visualization threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(sys.path[0]+\"/src/project_tools\")\n",
    "import nntools as nt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Centernet Class defined using the Neural Network class used during the assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Centernet_model(nt.NeuralNetwork,CtdetTrainer):\n",
    "    def __init__(self,opt,model,optimizer=None,FineTune=True):\n",
    "        nt.NeuralNetwork.__init__(self)\n",
    "        CtdetTrainer.__init__(self,opt,model,optimizer=None)\n",
    "        ## Partial Training of the Network \n",
    "        if FineTune:\n",
    "            for name,param in model.named_parameters():\n",
    "               if name[0:2]=='hm' or name[0:2]=='re' or name[0:2]=='wh':\n",
    "                    param.data = 0.1*torch.randn(param.size()) # Random initialization\n",
    "               else: \n",
    "                    param.requires_grad=False\n",
    "            #print(name,param.requires_grad)\n",
    "        self.model=model\n",
    "        self.opt=opt\n",
    "    def forward(self,x):\n",
    "        return self.model(x)\n",
    "    def criterion(self, y, d):\n",
    "        return self.loss(y,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading pretrained model https://download.pytorch.org/models/resnet18-5c106cde.pth\n",
      "=> init deconv weights from normal distribution\n",
      "loaded /datasets/home/home-01/30/230/psarangi/Final_proj/MoDL_CenterNet/models/ctdet_coco_resdcn18.pth, epoch 140\n"
     ]
    }
   ],
   "source": [
    "model = create_model(opt.arch, opt.heads, opt.head_conv) #Model creation with pre-trained weights\n",
    "model =load_model(model,sys.path[0]+'/models/ctdet_coco_resdcn18.pth');\n",
    "net = model;\n",
    "net = net.to(opt.device)\n",
    "net.eval(); #Set the network in eval mode for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = CtdetDetector(opt) #Detector class for performing object detection\n",
    "detector.model=net\n",
    "ret = detector.run(f_name)\n",
    "show_txt = True\n",
    "#Drawing Bounding Boxes\n",
    "for c_id in range(80):\n",
    "    for j in range(ret['results'][c_id+1].shape[0]):\n",
    "        if ret['results'][c_id+1][j][4]>=opt.vis_thresh:\n",
    "            bbox = ret['results'][c_id+1][j]\n",
    "            cv2.rectangle(I, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0,255,0), 2)\n",
    "            if show_txt:\n",
    "                txt = '{}{:.1f}'.format(coco_class_name[c_id], bbox[4]) # text+confidence\n",
    "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                cat_size = cv2.getTextSize(txt, font, 0.5, 2)[0]\n",
    "                cv2.rectangle(I, (bbox[0], int(bbox[1] - cat_size[1] - 2)),(int(bbox[0] + cat_size[0]), int(bbox[1] - 2)), (0,255,0), -1)\n",
    "                cv2.putText(I, txt, (bbox[0], int(bbox[1] - 2)), font, 0.5, (0, 0, 0),thickness=1, lineType=cv2.LINE_AA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = create_model(opt.arch, opt.heads, opt.head_conv)\n",
    "model2 =load_model(model2,sys.path[0]+'/models/ctdet_coco_resdcn18.pth');\n",
    "net2 = Centernet_model(opt,model2);\n",
    "net2 = net2.to(opt.device)\n",
    "#Load the baseline model trained with 2048 Training points-ResNet18 Backbone\n",
    "checkpoint2 = torch.load(sys.path[0]+'/models/baseline_resnet_2048.pth.tar')\n",
    "#print(checkpoint['Net'])\n",
    "net2.load_state_dict(checkpoint2['Net'])\n",
    "#net2.load_state_dict(torch.load(sys.path[0]+'/BaseExperiment/checkpoint.pth.tar'))\n",
    "net2.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opt.load_model = sys.path[0]+'/Experiment_upsamp2/checkpoint.pth.tar'\n",
    "I2 = io.imread(f_name)\n",
    "detector2 = CtdetDetector(opt)\n",
    "detector2.model=net2\n",
    "ret = detector2.run(f_name)\n",
    "show_txt = True\n",
    "#Drawing Bounding Boxes\n",
    "for c_id in range(80):\n",
    "    for j in range(ret['results'][c_id+1].shape[0]):\n",
    "        if ret['results'][c_id+1][j][4]>=opt.vis_thresh:\n",
    "            bbox = ret['results'][c_id+1][j]\n",
    "            cv2.rectangle(I2, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0,255,0), 2)\n",
    "            if show_txt:\n",
    "                txt = '{}{:.1f}'.format(coco_class_name[c_id], bbox[4]) # text+confidence\n",
    "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                cat_size = cv2.getTextSize(txt, font, 0.5, 2)[0]\n",
    "                cv2.rectangle(I2, (bbox[0], int(bbox[1] - cat_size[1] - 2)),(int(bbox[0] + cat_size[0]), int(bbox[1] - 2)), (0,255,0), -1)\n",
    "                cv2.putText(I2, txt, (bbox[0], int(bbox[1] - 2)), font, 0.5, (0, 0, 0),thickness=1, lineType=cv2.LINE_AA)\n",
    "\n",
    "\n",
    "#fig, axes = plt.subplots(figsize=(13,13))\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(20,20))\n",
    "#fig, axes = plt.subplots(nrows=1, ncols=2)\n",
    "axes[0].imshow(I)\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(I2)\n",
    "axes[1].axis('off')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 2: Architecture Demo (DLA vs Resnet Backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_name='2.jpg'\n",
    "f_name=img_dir+f_name\n",
    "I2 = io.imread(f_name)\n",
    "print(f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = detector2.run(f_name)\n",
    "show_txt = True\n",
    "#Drawing Bounding Boxes\n",
    "for c_id in range(80):\n",
    "    for j in range(ret['results'][c_id+1].shape[0]):\n",
    "        if ret['results'][c_id+1][j][4]>=opt.vis_thresh:\n",
    "            bbox = ret['results'][c_id+1][j]\n",
    "            cv2.rectangle(I2, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0,255,0), 2)\n",
    "            if show_txt:\n",
    "                txt = '{}{:.1f}'.format(coco_class_name[c_id], bbox[4]) # text+confidence\n",
    "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                cat_size = cv2.getTextSize(txt, font, 0.5, 2)[0]\n",
    "                cv2.rectangle(I2, (bbox[0], int(bbox[1] - cat_size[1] - 2)),(int(bbox[0] + cat_size[0]), int(bbox[1] - 2)), (0,255,0), -1)\n",
    "                cv2.putText(I2, txt, (bbox[0], int(bbox[1] - 2)), font, 0.5, (0, 0, 0),thickness=1, lineType=cv2.LINE_AA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "opt2 = fcn_opts(Dataset)\n",
    "opt2.arch='dla_34'\n",
    "opt2.head_conv = 256\n",
    "model3 = create_model(opt2.arch, opt2.heads, opt2.head_conv)\n",
    "net3 = Centernet_model(opt2,model3);\n",
    "net3 = net3.to(opt2.device)\n",
    "#print(sys.path[0]+'/Test_images/checkpoint.pth.tar')\n",
    "#Load the baseline model trained with 2048 Training points-DLA Backbone\n",
    "checkpoint = torch.load(sys.path[0]+'/models/baseline_dla_2048.pth.tar')\n",
    "#print(checkpoint['Net'])\n",
    "net3.load_state_dict(checkpoint['Net'])\n",
    "#net2.load_state_dict(torch.load(sys.path[0]+'/BaseExperiment/checkpoint.pth.tar'))\n",
    "net3.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opt.load_model = sys.path[0]+'/Experiment_upsamp2/checkpoint.pth.tar'\n",
    "opt2.load_model = sys.path[0]+'/models/ctdet_coco_dla_1x.pth'\n",
    "opt2.dataset = 'coco'\n",
    "opt2.debugger_theme = 'white'\n",
    "I3 = io.imread(f_name)\n",
    "detector3 = CtdetDetector(opt2)\n",
    "detector3.model=net3\n",
    "ret = detector3.run(f_name)\n",
    "show_txt = True\n",
    "#Drawing Bounding Boxes\n",
    "for c_id in range(80):\n",
    "    for j in range(ret['results'][c_id+1].shape[0]):\n",
    "        if ret['results'][c_id+1][j][4]>=opt2.vis_thresh:\n",
    "            bbox = ret['results'][c_id+1][j]\n",
    "            cv2.rectangle(I3, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0,255,0), 2)\n",
    "            if show_txt:\n",
    "                txt = '{}{:.1f}'.format(coco_class_name[c_id], bbox[4]) # text+confidence\n",
    "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                cat_size = cv2.getTextSize(txt, font, 0.5, 2)[0]\n",
    "                cv2.rectangle(I3, (bbox[0], int(bbox[1] - cat_size[1] - 2)),(int(bbox[0] + cat_size[0]), int(bbox[1] - 2)), (0,255,0), -1)\n",
    "                cv2.putText(I3, txt, (bbox[0], int(bbox[1] - 2)), font, 0.5, (0, 0, 0),thickness=1, lineType=cv2.LINE_AA)\n",
    "\n",
    "\n",
    "#fig, axes = plt.subplots(figsize=(13,13))\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(20,20))\n",
    "#fig, axes = plt.subplots(nrows=1, ncols=2)\n",
    "axes[0].imshow(I2)\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(I3)\n",
    "axes[1].axis('off')\n",
    "plt.show\n",
    "plt.savefig('dla_v_resnet.eps',format='eps',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 3 (Our Baseline model with 2048 training samples with and without Augmentation during training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_name='12.jpg'\n",
    "f_name=img_dir+f_name\n",
    "I2 = io.imread(f_name)\n",
    "(h, w) = I2.shape[:2]\n",
    "M = cv2.getRotationMatrix2D((w / 2, h / 2),25, 0.6)\n",
    "I2_ = cv2.warpAffine(I2, M, (w, h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = create_model(opt.arch, opt.heads, opt.head_conv)\n",
    "model2 =load_model(model2,sys.path[0]+'/models/ctdet_coco_resdcn18.pth');\n",
    "net2 = Centernet_model(opt,model2);\n",
    "net2 = net2.to(opt.device)\n",
    "#Load the baseline model trained with 2048 Training points-ResNet18 Backbone\n",
    "checkpoint2 = torch.load(sys.path[0]+'/models/baseline_resnet_2048.pth.tar')\n",
    "#print(checkpoint['Net'])\n",
    "net2.load_state_dict(checkpoint2['Net'])\n",
    "net2.eval();\n",
    "detector = CtdetDetector(opt)\n",
    "detector.model=net2\n",
    "ret = detector.run(I2_)\n",
    "show_txt = True\n",
    "#Drawing Bounding Boxes\n",
    "for c_id in range(80):\n",
    "    for j in range(ret['results'][c_id+1].shape[0]):\n",
    "        if ret['results'][c_id+1][j][4]>=opt.vis_thresh:\n",
    "            bbox = ret['results'][c_id+1][j]\n",
    "            cv2.rectangle(I2_, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0,255,0), 2)\n",
    "            if show_txt:\n",
    "                txt = '{}{:.1f}'.format(coco_class_name[c_id], bbox[4]) # text+confidence\n",
    "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                cat_size = cv2.getTextSize(txt, font, 0.5, 2)[0]\n",
    "                cv2.rectangle(I2_, (bbox[0], int(bbox[1] - cat_size[1] - 2)),(int(bbox[0] + cat_size[0]), int(bbox[1] - 2)), (0,255,0), -1)\n",
    "                cv2.putText(I2_, txt, (bbox[0], int(bbox[1] - 2)), font, 0.5, (0, 0, 0),thickness=1, lineType=cv2.LINE_AA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = create_model(opt.arch, opt.heads, opt.head_conv)\n",
    "net4 = Centernet_model(opt,model3);\n",
    "net4 = net4.to(opt.device)\n",
    "##Load the baseline model trained with 2048 Training points- But no Augmentation such as flip, rotation or scaling\n",
    "checkpoint = torch.load(sys.path[0]+'/models/no_aug_model.pth.tar')\n",
    "net4.load_state_dict(checkpoint['Net'])\n",
    "net4.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a smal Rotation and scaling transformation leads to misidentification with the same visualization threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I3 = io.imread(f_name)\n",
    "I3_ = cv2.warpAffine(I3, M, (w, h))\n",
    "detector2 = CtdetDetector(opt)\n",
    "detector2.model=net4\n",
    "ret = detector2.run(I3_)\n",
    "show_txt = True\n",
    "#Drawing Bounding Boxes\n",
    "for c_id in range(80):\n",
    "    for j in range(ret['results'][c_id+1].shape[0]):\n",
    "        if ret['results'][c_id+1][j][4]>=opt.vis_thresh:\n",
    "            bbox = ret['results'][c_id+1][j]\n",
    "            cv2.rectangle(I3_, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0,255,0), 2)\n",
    "            if show_txt:\n",
    "                txt = '{}{:.1f}'.format(coco_class_name[c_id], bbox[4]) # text+confidence\n",
    "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                cat_size = cv2.getTextSize(txt, font, 0.5, 2)[0]\n",
    "                cv2.rectangle(I3_, (bbox[0], int(bbox[1] - cat_size[1] - 2)),(int(bbox[0] + cat_size[0]), int(bbox[1] - 2)), (0,255,0), -1)\n",
    "                cv2.putText(I3_, txt, (bbox[0], int(bbox[1] - 2)), font, 0.5, (0, 0, 0),thickness=1, lineType=cv2.LINE_AA)\n",
    "\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(20,20))\n",
    "axes[0].imshow(I2_)\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(I3_)\n",
    "axes[1].axis('off')\n",
    "plt.show\n",
    "plt.savefig('aug.eps',format='eps',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 4: Upsampling Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_name='10.jpg'\n",
    "f_name=img_dir+f_name\n",
    "I2 = io.imread(f_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illustrating small object detection (Tie) which was not recognized by base network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = fcn_opts(Dataset)\n",
    "opt.load_model = sys.path[0]+'/models/ctdet_coco_resdcn18.pth'\n",
    "opt.vis_thresh=0.2\n",
    "model = create_model(opt.arch, opt.heads, opt.head_conv)\n",
    "net2 = Centernet_model(opt,model);\n",
    "net2 = net2.to(opt.device)\n",
    "#Load the baseline model trained with 2048 Training points-ResNet18 Backbone\n",
    "checkpoint = torch.load(sys.path[0]+'/models/baseline_resnet_2048.pth.tar')\n",
    "net2.load_state_dict(checkpoint['Net'])\n",
    "net2.eval();\n",
    "detector = CtdetDetector(opt)\n",
    "detector.model=net2\n",
    "ret = detector.run(f_name)\n",
    "show_txt = True\n",
    "#Drawing Bounding Boxes for specific class = Tie\n",
    "c_id=27\n",
    "for j in range(ret['results'][c_id+1].shape[0]):\n",
    "    if ret['results'][c_id+1][j][4]>=opt.vis_thresh:\n",
    "        bbox = ret['results'][c_id+1][j]\n",
    "        cv2.rectangle(I2, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0,255,0), 2)\n",
    "        if show_txt:\n",
    "            txt = '{}{:.1f}'.format(coco_class_name[c_id], bbox[4]) # text+confidence\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            cat_size = cv2.getTextSize(txt, font, 0.5, 2)[0]\n",
    "            cv2.rectangle(I2, (bbox[0], int(bbox[1] - cat_size[1] - 2)),(int(bbox[0] + cat_size[0]), int(bbox[1] - 2)), (0,255,0), -1)\n",
    "            cv2.putText(I2, txt, (bbox[0], int(bbox[1] - 2)), font, 0.5, (0, 0, 0),thickness=1, lineType=cv2.LINE_AA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net5 = Centernet_model(opt,model);\n",
    "net5 = net5.to(opt.device)\n",
    "#Load the model trained with 2048 Training and With High-resolution Object detection output\n",
    "#Objective: Show better detection of smaller scale objects\n",
    "checkpoint = torch.load(sys.path[0]+'/models/upsample_model.pth.tar')\n",
    "net5.load_state_dict(checkpoint['Net'])\n",
    "net5.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I3 = io.imread(f_name)\n",
    "detector2 = CtdetDetector(opt)\n",
    "detector2.model=net5\n",
    "ret = detector2.run(f_name)\n",
    "show_txt = True\n",
    "#Drawing Bounding Boxes for specific class = Tie\n",
    "#c_id=27\n",
    "for j in range(ret['results'][c_id+1].shape[0]):\n",
    "    if ret['results'][c_id+1][j][4]>=opt.vis_thresh:\n",
    "        bbox = ret['results'][c_id+1][j]\n",
    "        cv2.rectangle(I3, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0,255,0), 2)\n",
    "        if show_txt:\n",
    "            txt = '{}{:.1f}'.format(coco_class_name[c_id], bbox[4]) # text+confidence\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            cat_size = cv2.getTextSize(txt, font, 0.5, 2)[0]\n",
    "            cv2.rectangle(I3, (bbox[0], int(bbox[1] - cat_size[1] - 2)),(int(bbox[0] + cat_size[0]), int(bbox[1] - 2)), (0,255,0), -1)\n",
    "            cv2.putText(I3, txt, (bbox[0], int(bbox[1] - 2)), font, 0.5, (0, 0, 0),thickness=1, lineType=cv2.LINE_AA)\n",
    "\n",
    "\n",
    "#fig, axes = plt.subplots(figsize=(13,13))\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(20,20))\n",
    "#fig, axes = plt.subplots(nrows=1, ncols=2)\n",
    "axes[0].imshow(I2)\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(I3)\n",
    "axes[1].axis('off')\n",
    "plt.show\n",
    "plt.savefig('small.eps',format='eps',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = fcn_opts(Dataset)\n",
    "opt.load_model = sys.path[0]+'/models/ctdet_coco_resdcn18.pth'\n",
    "opt.vis_thresh=0.2\n",
    "model = create_model(opt.arch, opt.heads, opt.head_conv)\n",
    "net2 = Centernet_model(opt,model);\n",
    "net2 = net2.to(opt.device)\n",
    "checkpoint = torch.load(sys.path[0]+'/models/baseline_resnet_2048.pth.tar')\n",
    "net2.load_state_dict(checkpoint['Net'])\n",
    "net2.eval();\n",
    "detector = CtdetDetector(opt)\n",
    "detector.model=net2\n",
    "ret = detector.run(f_name)\n",
    "show_txt = True\n",
    "#Drawing Bounding Boxes\n",
    "for c_id in range(80):\n",
    "    for j in range(ret['results'][c_id+1].shape[0]):\n",
    "        if ret['results'][c_id+1][j][4]>=opt.vis_thresh:\n",
    "            bbox = ret['results'][c_id+1][j]\n",
    "            cv2.rectangle(I2, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0,255,0), 2)\n",
    "            if show_txt:\n",
    "                txt = '{}{:.1f}'.format(coco_class_name[c_id], bbox[4]) # text+confidence\n",
    "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                cat_size = cv2.getTextSize(txt, font, 0.5, 2)[0]\n",
    "                cv2.rectangle(I2, (bbox[0], int(bbox[1] - cat_size[1] - 2)),(int(bbox[0] + cat_size[0]), int(bbox[1] - 2)), (0,255,0), -1)\n",
    "                cv2.putText(I2, txt, (bbox[0], int(bbox[1] - 2)), font, 0.5, (0, 0, 0),thickness=1, lineType=cv2.LINE_AA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I3 = io.imread(f_name)\n",
    "detector2 = CtdetDetector(opt)\n",
    "detector2.model=net5\n",
    "ret = detector2.run(f_name)\n",
    "show_txt = True\n",
    "#Drawing Bounding Boxes for specific class = Tie\n",
    "for c_id in range(80):\n",
    "    for j in range(ret['results'][c_id+1].shape[0]):\n",
    "        if ret['results'][c_id+1][j][4]>=opt.vis_thresh:\n",
    "            bbox = ret['results'][c_id+1][j]\n",
    "            cv2.rectangle(I3, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0,255,0), 2)\n",
    "            if show_txt:\n",
    "                txt = '{}{:.1f}'.format(coco_class_name[c_id], bbox[4]) # text+confidence\n",
    "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                cat_size = cv2.getTextSize(txt, font, 0.5, 2)[0]\n",
    "                cv2.rectangle(I3, (bbox[0], int(bbox[1] - cat_size[1] - 2)),(int(bbox[0] + cat_size[0]), int(bbox[1] - 2)), (0,255,0), -1)\n",
    "                cv2.putText(I3, txt, (bbox[0], int(bbox[1] - 2)), font, 0.5, (0, 0, 0),thickness=1, lineType=cv2.LINE_AA)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(20,20))\n",
    "axes[0].imshow(I2)\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(I3)\n",
    "axes[1].axis('off')\n",
    "plt.show\n",
    "plt.savefig('small2.eps',format='eps',bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
